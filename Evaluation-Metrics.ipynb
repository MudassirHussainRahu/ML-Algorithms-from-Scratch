{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7658bb7b-5f20-4148-92a3-93e001c3cfb6",
   "metadata": {},
   "source": [
    "### 🧮 Classification Accuracy\n",
    "\n",
    "**Definition:**  \n",
    "Classification accuracy measures how often a classification model correctly predicts the class labels.\n",
    "\n",
    "---\n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}} \\times 100\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Example:**  \n",
    "If your model made **100 predictions**, and **85** of them were correct:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{85}{100} \\times 100 = 85\\%\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86abd24-5101-4a9e-bceb-e54f60b33da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for label, prediction in  zip(actual, predicted):\n",
    "        if label == prediction:\n",
    "            correct += 1\n",
    "    return (correct/len(actual))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca7f2c6-9840-4ec8-8670-21f9aa6998cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n"
     ]
    }
   ],
   "source": [
    "actual = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,0,0,0,1,0,1,1,1]\n",
    "\n",
    "accuracy = accuracy_metric(actual, predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360f7fc-4bfa-4e1b-8584-2a5709bd789a",
   "metadata": {},
   "source": [
    "### 4.2.2 Confusion Matrix\n",
    "\n",
    "A **confusion matrix** provides a summary of all predictions made by a classification model compared to the expected (actual) values.  \n",
    "It is represented as a table (matrix) showing counts of predictions versus actual outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Each **row** of the matrix represents the **predicted** class.\n",
    "- Each **column** represents the **actual** class.\n",
    "- The **diagonal elements** (from top-left to bottom-right) indicate **correct predictions**.\n",
    "- Off-diagonal elements show **misclassifications**.\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "A perfect classifier will have all predictions along the **main diagonal**, meaning the predicted labels match the actual labels for every instance.\n",
    "\n",
    "**Example Confusion Matrix:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "50 & 2 & 0 \\\\\n",
    "1 & 45 & 4 \\\\\n",
    "0 & 3 & 47\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30136f67-2b70-4498-965d-e4b2c905de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(actual, predicted):\n",
    "    unique = set(actual)\n",
    "    matrix = [ list() for x in range(len(unique)) ]\n",
    "    for i in range(len(unique)):\n",
    "        matrix[i] = [ 0 for x in range(len(unique)) ]\n",
    "\n",
    "    lookup = dict()\n",
    "\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "\n",
    "    for i in range(len(actual)):\n",
    "        x = lookup[actual[i]]\n",
    "        y = lookup[predicted[i]]\n",
    "\n",
    "        matrix[y][x] += 1\n",
    "\n",
    "    print('(A)'+ ' '.join(str(x) for x in unique))\n",
    "    print( '(P)---------------' )\n",
    "    for i, x in enumerate(unique):\n",
    "        print(\"%s| %s\"%(x, \" \".join(str(x) for x in matrix[i])))\n",
    "    \n",
    "    # return unique, matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "816162cc-65b8-4e98-a05b-977cc2cf94b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A)0 1\n",
      "(P)---------------\n",
      "0| 3 1\n",
      "1| 2 4\n"
     ]
    }
   ],
   "source": [
    "actual = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,1,0,0,1,0,1,1,1]\n",
    "confusion_matrix(actual, predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81170aa3-1708-4512-970c-3d1f6bf37dea",
   "metadata": {},
   "source": [
    "### 4.2.3 Mean Absolute Error (MAE)\n",
    "\n",
    "**Definition:**  \n",
    "Regression problems involve predicting continuous (real) values.  \n",
    "A simple and intuitive evaluation metric for regression is the **Mean Absolute Error (MAE)**,  \n",
    "which measures the average magnitude of the errors between predicted and actual values.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**  \n",
    "- The **error** is the difference between the predicted and actual values.  \n",
    "- The **absolute value** of each error is taken to avoid negative values canceling positive ones.  \n",
    "- The MAE is then the **average** of these absolute errors.\n",
    "\n",
    "---\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\text{predicted}_i - \\text{actual}_i \\right|\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ n $ = total number of predictions  \n",
    "- $ \\text{predicted}_i $ = predicted value for the $ i^{th} $ observation  \n",
    "- $ \\text{actual}_i $ = actual (true) value for the $ i^{th} $ observation  \n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:**  \n",
    "A lower MAE value indicates that the predictions are closer to the actual values, meaning better model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76f660eb-0d25-43be-b8d5-d23deb7ad1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for a, p in zip(actual, predicted):\n",
    "        sum_error += abs(a-p)\n",
    "\n",
    "    return sum_error/float(len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "981a0a67-efc5-4d56-a501-282f62f2bc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007999999999999993\n"
     ]
    }
   ],
   "source": [
    "actual = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "predicted = [0.11, 0.19, 0.29, 0.41, 0.5]\n",
    "mae = mae_metric(actual, predicted)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08966e0-4189-487b-9b23-a5c14a9736ba",
   "metadata": {},
   "source": [
    "### 4.2.4 Root Mean Squared Error (RMSE)\n",
    "\n",
    "**Definition:**  \n",
    "Another widely used metric for evaluating regression models is the **Root Mean Squared Error (RMSE)**.  \n",
    "It measures the average magnitude of the squared differences between the predicted and actual values.  \n",
    "Sometimes, the metric is referred to as **Mean Squared Error (MSE)** when the square root is not taken.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**  \n",
    "- Each prediction error (difference between predicted and actual) is **squared** to remove negative signs.  \n",
    "- The **mean** of these squared errors is computed.  \n",
    "- Finally, the **square root** of that mean gives RMSE, bringing the error measure back to the original units.\n",
    "\n",
    "---\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (\\text{predicted}_i - \\text{actual}_i)^2 }\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\( n \\) = total number of predictions  \n",
    "- \\( \\text{predicted}_i \\) = predicted value for the \\( i^{th} \\) observation  \n",
    "- \\( \\text{actual}_i \\) = actual (true) value for the \\( i^{th} \\) observation  \n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:**  \n",
    "- RMSE gives a higher weight to large errors (due to squaring), making it more sensitive to outliers.  \n",
    "- A **lower RMSE** indicates better model performance and predictions closer to actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84cbcc13-6f2f-4d59-8ccb-7a84f15d8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = 0\n",
    "    for i in range(len(actual)):\n",
    "        sum_error = sum_error + (actual[i]- predicted[i])**2\n",
    "    mean_error = sum_error/len(actual)\n",
    "\n",
    "    return sqrt(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25510cf5-2a30-4447-bd09-84e4e0403099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00894427190999915\n"
     ]
    }
   ],
   "source": [
    "actual = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "predicted = [0.11, 0.19, 0.29, 0.41, 0.5]\n",
    "rmse = rmse_metric(actual, predicted)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967622db-0bc4-45d5-a4f5-35549a9c1e29",
   "metadata": {},
   "source": [
    "### 4.2.5 Precision for Classification\n",
    "\n",
    "**Definition:**  \n",
    "Precision is an important evaluation metric for classification problems, especially when the cost of false positives is high.  \n",
    "It measures how many of the instances predicted as *positive* are actually *positive*.  \n",
    "In other words, it answers the question:  \n",
    "*\"Of all the samples the model labeled as positive, how many were correct?\"*\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**  \n",
    "Precision focuses only on the **positive predictions** made by the model and helps assess how reliable those predictions are.  \n",
    "It is particularly useful in applications such as **spam detection**, **medical diagnosis**, or **fraud detection**,  \n",
    "where incorrectly predicting a positive result (false positive) can have serious consequences.\n",
    "\n",
    "---\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- **True Positives (TP):** Correctly predicted positive instances  \n",
    "- **False Positives (FP):** Incorrectly predicted positive instances  \n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:**  \n",
    "A higher precision value indicates that the model produces fewer false positives,  \n",
    "meaning its positive predictions are more **accurate and trustworthy**.  \n",
    "Precision is often used together with **Recall** to provide a more complete evaluation of classification performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c91fc236-ebe0-4536-824a-035c5ff028ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_metric_binary(actual, predicted):\n",
    "    true_positives = 0\n",
    "    false_positives =0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i] and actual[i] == 1:\n",
    "            true_positives += 1\n",
    "        elif actual[i] != predicted[i] and predicted[i] == 1:\n",
    "            false_positives += 1\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    return true_positives/ ( true_positives + false_positives )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4efb3c50-5dcc-4676-be0d-db6485192a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "actual = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,1,0,0,1,0,1,1,1]\n",
    "\n",
    "print(precision_metric_binary(actual,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e39065-3dcb-4617-8b5c-f7b290b73a2b",
   "metadata": {},
   "source": [
    "## 2. Multi-Class (Categorical) Classification\n",
    "\n",
    "For categorical problems (with 3 or more classes), **precision** is computed per class, treating each class as the “positive” class while others are “negative.”\n",
    "\n",
    "Then, you combine these class-wise precisions using one of the following approaches:\n",
    "\n",
    "---\n",
    "\n",
    "### a) **Macro Precision**\n",
    "\n",
    "**Formula:**  \n",
    "$ \\text{Macro Precision} = \\frac{1}{K} \\sum_{i=1}^{K} \\text{Precision}_i $\n",
    "\n",
    "- Calculates precision for each class independently  \n",
    "- Takes the **simple average** (treats all classes equally)\n",
    "\n",
    "---\n",
    "\n",
    "### b) **Weighted Precision**\n",
    "\n",
    "**Formula:**  \n",
    "$ \\text{Weighted Precision} = \\frac{\\sum_{i=1}^{K} n_i \\times \\text{Precision}_i}{\\sum_{i=1}^{K} n_i} $\n",
    "\n",
    "- Each class’s precision is **weighted by how many samples it has**  \n",
    "- Useful when classes are **imbalanced**\n",
    "\n",
    "---\n",
    "\n",
    "### c) **Micro Precision**\n",
    "\n",
    "**Formula:**  \n",
    "$ \\text{Micro Precision} = \\frac{\\sum_{i=1}^{K} TP_i}{\\sum_{i=1}^{K} (TP_i + FP_i)} $\n",
    "\n",
    "- Aggregates **True Positives (TP)** and **False Positives (FP)** across all classes before computing precision  \n",
    "- Gives **more weight to larger classes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95326fe6-5916-4f0d-9584-9bfa65acd3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_metric_categorical(actual, predicted, method):\n",
    "    unique = set(actual)\n",
    "    true_positives = [ 0 for x in range(len(unique)) ]\n",
    "    false_positives = [ 0 for x in range(len(unique)) ]\n",
    "    precisions = [ 0 for x in range(len(unique)) ]\n",
    "    lookup = dict()\n",
    "        \n",
    "    for i, v in enumerate(unique):\n",
    "        lookup[v] = i\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            index = lookup[actual[i]]\n",
    "            true_positives[index] += 1\n",
    "        else:\n",
    "            index = lookup[predicted[i]]\n",
    "            false_positives[index] += 1\n",
    "\n",
    "    for v in unique:\n",
    "        prec = true_positives[lookup[v]]/(true_positives[lookup[v]] + false_positives[lookup[v]])\n",
    "        precisions[lookup[v]] = prec\n",
    "    print(precisions)\n",
    "    \n",
    "    if method==\"macro\":\n",
    "        return sum(precisions)/len(unique)\n",
    "        \n",
    "    elif method == \"weighted\":\n",
    "        weighted_precision_sum = 0\n",
    "        for label in unique:\n",
    "            number_of_labels = actual.count(label)\n",
    "            weighted_precision_sum += (number_of_labels*precisions[lookup[label]])\n",
    "        total_samples = sum(actual.count(label) for label in unique)\n",
    "        return weighted_precision_sum/total_samples\n",
    "\n",
    "    elif method == \"micro\":\n",
    "        sum_of_true_positives = sum(true_positives)\n",
    "        sum_of_false_positives = sum(false_positives)\n",
    "        return sum_of_true_positives/(sum_of_true_positives+sum_of_false_positives)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a11984f-dbd0-488e-bc3d-327de667a4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6, 0.6, 0.6, 0.8]\n",
      "[0.6, 0.6, 0.6, 0.8]\n",
      "[0.6, 0.6, 0.6, 0.8]\n",
      "Macro Precision   : 0.6500\n",
      "Weighted Precision: 0.6500\n",
      "Micro Precision   : 0.6500\n"
     ]
    }
   ],
   "source": [
    "# --- Example test data ---\n",
    "actual =    [0,0,0,1,1,1,2,2,2,3,3,3,0,1,2,3,0,1,2,3]\n",
    "predicted = [0,1,0,1,1,2,2,0,2,3,2,3,1,1,2,3,0,0,3,3]\n",
    "\n",
    "# --- Evaluate all methods ---\n",
    "macro_precision = precision_metric_categorical(actual, predicted, method=\"macro\")\n",
    "weighted_precision = precision_metric_categorical(actual, predicted, method=\"weighted\")\n",
    "micro_precision = precision_metric_categorical(actual, predicted, method=\"micro\")\n",
    "\n",
    "# --- Print results ---\n",
    "print(f\"Macro Precision   : {macro_precision:.4f}\")\n",
    "print(f\"Weighted Precision: {weighted_precision:.4f}\")\n",
    "print(f\"Micro Precision   : {micro_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9bf3a-f808-4d20-8b28-d55c29c5d7e9",
   "metadata": {},
   "source": [
    "### 4.2.5 Recall for Classification\n",
    "\n",
    "Recall is a common evaluation metric used in **classification problems**, particularly when it is important to correctly identify all positive instances.  \n",
    "It measures the proportion of **actual positive samples** that are correctly predicted by the model.\n",
    "\n",
    "In other words, Recall answers the question:\n",
    "\n",
    "> *“Out of all the true positive cases, how many did the model successfully identify?”*\n",
    "\n",
    "A high recall indicates that most positive instances were captured by the model,  \n",
    "while a low recall suggests that many positives were missed.\n",
    "\n",
    "Mathematically, Recall is defined as:\n",
    "\n",
    "**Recall =** $ \\frac{TP}{TP + FN} $\n",
    "\n",
    "where:  \n",
    "- **TP (True Positives)** → Number of positive samples correctly classified as positive  \n",
    "- **FN (False Negatives)** → Number of positive samples incorrectly classified as negative  \n",
    "\n",
    "The value of Recall ranges from **0 to 1**, where:  \n",
    "- **1** → Perfect recall (no positive samples were missed)  \n",
    "- **0** → Model failed to identify any positive samples  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Difference Between Accuracy, Precision, and Recall\n",
    "\n",
    "| Metric | Formula | Measures | Best Used When |\n",
    "|:-------|:---------|:----------|:----------------|\n",
    "| **Accuracy** | $  \\frac{TP + TN}{TP + TN + FP + FN}  $| Overall correctness of the model | When all classes are equally important |\n",
    "| **Precision** | $ \\frac{TP}{TP + FP} $ | How many predicted positives are actually positive | When the cost of a false positive is high |\n",
    "| **Recall** | $ \\frac{TP}{TP + FN} $ | How many actual positives are correctly identified | When the cost of missing a positive is high |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Confusion Matrix (Conceptual)\n",
    "\n",
    "|                | **Predicted Positive** | **Predicted Negative** |\n",
    "|:----------------|:----------------------:|:----------------------:|\n",
    "| **Actual Positive** | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "---\n",
    "\n",
    "**Example:**  \n",
    "In medical diagnosis, **Recall** is more important than Precision because it’s critical to identify *all* patients who actually have the disease, even if some healthy individuals are incorrectly flagged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ac886e6-5e64-48d3-963d-00fd21fdd4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(actual,predicted):\n",
    "    unique = set(actual)\n",
    "    true_positives = [ 0 for x in range(len(unique)) ]\n",
    "    false_negatives = [ 0 for x in range(len(unique)) ]\n",
    "    lookup = dict()\n",
    "    for i, v in enumerate(unique):\n",
    "        lookup[v] = i\n",
    "\n",
    "    for i in range(len(actual)):\n",
    "        index = lookup[actual[i]]\n",
    "        if actual[i] == predicted[i]:\n",
    "            true_positives[index] += 1\n",
    "        else:\n",
    "            false_negatives[index] += 1\n",
    "\n",
    "    recalls = []\n",
    "    for label in unique:\n",
    "        index = lookup[label]\n",
    "        tp = true_positives[index]\n",
    "        fn = false_negatives[index]\n",
    "        recalls.append(tp/(tp+fn))\n",
    "    return recalls\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1e3e920-1636-40d6-a429-c5153ea3f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6, 0.6, 0.6, 0.8]\n"
     ]
    }
   ],
   "source": [
    "actual =    [0,0,0,1,1,1,2,2,2,3,3,3,0,1,2,3,0,1,2,3]\n",
    "predicted = [0,1,0,1,1,2,2,0,2,3,2,3,1,1,2,3,0,0,3,3]\n",
    "\n",
    "print(recall(actual, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1556f8-d339-4de7-94cd-13dea0e3a1cc",
   "metadata": {},
   "source": [
    "### 4.2.6 F1 Score for Classification\n",
    "\n",
    "The **F1 Score** is an important evaluation metric for **classification problems**, especially when there is an **imbalance between classes**.  \n",
    "It combines both **Precision** and **Recall** into a single metric by taking their harmonic mean.\n",
    "\n",
    "While **Precision** measures *how many predicted positives are actually positive*, and **Recall** measures *how many actual positives were correctly identified*,  \n",
    "the **F1 Score** balances both — giving a better sense of the model’s overall effectiveness.\n",
    "\n",
    "Mathematically, the F1 Score is defined as:\n",
    "\n",
    "**F1 Score =** $  2 \\times \\frac{(\\text{Precision} \\times \\text{Recall})}{(\\text{Precision} + \\text{Recall})}  $\n",
    "\n",
    "The value of F1 ranges from **0 to 1**, where:  \n",
    "- **1** → Perfect precision and recall (best performance)  \n",
    "- **0** → Poor performance (either precision or recall is zero)\n",
    "\n",
    "---\n",
    "\n",
    "**Key Points:**\n",
    "- F1 is more useful than Accuracy for **imbalanced datasets**.  \n",
    "- It penalizes models that perform well on one metric (e.g., precision) but poorly on the other (e.g., recall).  \n",
    "- It is commonly used in information retrieval, fraud detection, and medical diagnosis tasks.\n",
    "\n",
    "---\n",
    "\n",
    "| Metric | Balances | Best Used When |\n",
    "|:-------|:-----------|:----------------|\n",
    "| **F1 Score** | Precision and Recall | When both false positives and false negatives are costly |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70a773-c336-44aa-accc-72ac7dd8fd15",
   "metadata": {},
   "source": [
    "### 4.2.7 Area Under ROC Curve or AUC for Classification\n",
    "\n",
    "The **Area Under the Receiver Operating Characteristic (ROC) Curve**, or **AUC**, is a powerful evaluation metric for **binary classification** problems.  \n",
    "It measures how well the model can **distinguish between classes** — that is, how well it separates positive and negative samples.\n",
    "\n",
    "The **ROC Curve** plots:  \n",
    "- **True Positive Rate (TPR)** on the Y-axis, and  \n",
    "- **False Positive Rate (FPR)** on the X-axis.\n",
    "\n",
    "Where:  \n",
    "- $ \\text{TPR} = \\frac{TP}{TP + FN} $ (Recall)  \n",
    "- $ \\text{FPR} = \\frac{FP}{FP + TN} $\n",
    "\n",
    "The **AUC** represents the **area under this curve**, providing a single value summary of the model’s performance.\n",
    "\n",
    "**Interpretation:**\n",
    "- **AUC = 1.0** → Perfect classifier  \n",
    "- **AUC = 0.5** → No discrimination (random guessing)  \n",
    "- **AUC < 0.5** → Worse than random guessing\n",
    "\n",
    "---\n",
    "\n",
    "**Key Points:**\n",
    "- AUC is **threshold-independent**, meaning it measures performance across all possible decision thresholds.  \n",
    "- It’s useful when comparing models with different probability calibration.  \n",
    "- Works best for **binary classification**; can be extended to multi-class via **one-vs-rest (OvR)** or **macro averaging**.\n",
    "\n",
    "---\n",
    "\n",
    "| AUC Range | Interpretation |\n",
    "|:-----------|:----------------|\n",
    "| 0.9 – 1.0 | Excellent |\n",
    "| 0.8 – 0.9 | Good |\n",
    "| 0.7 – 0.8 | Fair |\n",
    "| 0.6 – 0.7 | Poor |\n",
    "| 0.5 | Random |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0905de-9027-4e95-8b48-f746bd5955cc",
   "metadata": {},
   "source": [
    "### 4.2.8 Goodness of Fit (R² Score) for Regression\n",
    "\n",
    "The **R-squared (R²)**, also known as the **Coefficient of Determination**, is a key metric used to evaluate **regression models**.  \n",
    "It measures how well the model’s predictions approximate the actual data points.\n",
    "\n",
    "In simple terms, R² shows the **proportion of variance** in the dependent variable that is explained by the independent variables.\n",
    "\n",
    "Mathematically, R² is defined as:\n",
    "\n",
    "**R² =** $ 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} $\n",
    "\n",
    "where:  \n",
    "- $ y_i $ → Actual value  \n",
    "- $ \\hat{y}_i $ → Predicted value  \n",
    "- $ \\bar{y} $ → Mean of actual values  \n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:**\n",
    "- **R² = 1** → Perfect fit (model explains all variance)  \n",
    "- **R² = 0** → Model explains no variance (predictions are as good as the mean)  \n",
    "- **R² < 0** → Model performs worse than simply predicting the mean\n",
    "\n",
    "---\n",
    "\n",
    "**Key Points:**\n",
    "- A higher R² indicates a better model fit.  \n",
    "- However, R² alone doesn’t indicate if a model is appropriate — it may increase with more predictors even if they’re irrelevant.  \n",
    "- Adjusted R² is preferred when comparing models with different numbers of predictors.\n",
    "\n",
    "---\n",
    "\n",
    "| R² Value | Interpretation |\n",
    "|:----------|:----------------|\n",
    "| 0.9 – 1.0 | Excellent fit |\n",
    "| 0.7 – 0.9 | Good fit |\n",
    "| 0.5 – 0.7 | Moderate fit |\n",
    "| < 0.5 | Poor fit |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f821d-d09d-4889-908b-11caa1913c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
